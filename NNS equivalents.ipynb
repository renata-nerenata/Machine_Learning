{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a two-layer network function of the form\n",
    "    \\begin{equation}\n",
    "    y_k(x, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=1}^M w_{kj}^{(2)} \\sigma \\left(\\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right)\n",
    "                               + w_{k0}^{(2)}\\right)\n",
    "    \\end{equation}\n",
    "in which the nonlinear activation functions are logistic sigmoid functions\n",
    "    \\begin{equation}\n",
    "    \\sigma(a) = (1 + \\exp(−a))^{-1}.\n",
    "    \\end{equation}\n",
    "Show that there exists an equivalent network, which computes exactly the same function but with hidden unit activation function given by hyperbolic tangent ${\\rm tanh}(a)$\n",
    "    \\begin{equation}\n",
    "    {\\rm tanh}(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Relation between logistic sigmoid and tanh:\n",
    "\n",
    "$${\\rm tanh}(a) = {\\rm tanh}(a) + 1 - 1  = \\frac{e^a - e^{-a}}{e^a + e^{-a}} + \\frac{e^a + e^{-a}}{e^a + e^{-a}} - 1 = \\frac{2e^a}{e^a + e^{-a}} - 1 = \\frac{2}{1+e^{-2a}}- 1$$\n",
    "\n",
    "That means:\n",
    "$$ {\\rm tanh}(a) = 2\\sigma(2a) -1 \\Rightarrow \\sigma(a) =  \\frac{{\\rm tanh}(a/2)}{2} +\\frac{1}{2}$$\n",
    "\n",
    "To get the desired result you need:\n",
    "Based on the formula: Our weights with a sigmoid activation function, multiply the input-hidden weight by 1/2 and divide by half and add 1/2\n",
    " \n",
    "\n",
    "Thus, we can subtitute $g(a)$ for $\\frac{1}{2} (1 - \\tanh(a/2))$ and plug it into equation $(1)$:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "    y_k (x, \\mathbf{w}) = \\sigma \\left( \\sum_{j=1}^M w_{kj}^{(2)}  \\left( \\frac{1}{2} - \\tanh (\\frac{1}{2} \\sum_{i=1}^D w_{ji}^{(1)} x_i + \\frac{1}{2} w_{j0}^{(1)} ) \\right) + w_{k0}^{(2)} \\right), \\quad (2)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Now, simplify this equation:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    y_k (x, \\mathbf{w}) = \\sigma \\left( \\frac{\\sum_{j=1}^M w_{kj}^{(2)}}{2} - \\sum_{j=1}^M w_{kj}^{(2)} \\tanh (\\frac{1}{2} \\sum_{i=1}^D w_{ji}^{(1)} x_i + \\frac{1}{2} w_{j0}^{(1)} ) + w_{k0}^{(2)} \\right), \\quad (3)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Introduce new weights $\\tilde{w}$:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    & \\tilde{w_{k0}}^{(2)} = \\frac{\\sum_{j=1}^M w_{kj}^{(2)}}{2} + w_{k0}^{(2)}, \\\\\n",
    "    & \\tilde{w_{kj}}^{(2)} = -w_{kj}^{(2)}, \\\\\n",
    "    & \\tilde{w_{j0}}^{(1)} = \\frac{1}{2} w_{j0}^{(1)},\\\\\n",
    "    & \\tilde{w_{ji}}^{(1)} = \\frac{1}{2} w_{ji}^{(1)}.\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Finally, we have the next equation:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    y_k (x, \\mathbf{w}) = \\sigma \\left( \\sum_{j=1}^M \\tilde{w_{kj}}^{(2)} \\tanh \\left( \\sum_{i=1}^D \\tilde{w_{ji}}^{(1)} x_i + \\tilde{w_{j0}}^{(1)} \\right) + \\tilde{w_{k0}}^{(2)} \\right). \\quad (4)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6*. Data augmentation (2 pt.)\n",
    "One way to encourage invariance of a model w.r.t a set of transformations is to expand the training set using transformed versions of the original input patterns.\n",
    "Consider the framework for training with transformed data in the special case when the transformation is simply addition of random noise $x \\rightarrow x + \\xi$ where $\\xi$ has a Gaussian distribution with zero mean and unit variance. The error function for untransformed inputs can be written (in the infinite data set limit) in the form\n",
    "    \\begin{equation}\n",
    "    E = \\frac12 \\int \\int (y(\\mathbf{x}) - t)^2 p(t|\\mathbf{x}) p(\\mathbf{x}){\\rm d}\\mathbf{x} {\\rm d}t.\n",
    "    \\end{equation}\n",
    "If we now consider an infinite number of copies of each data point perturbed by the transformation, then the error function can be written as\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} = \\frac12 \\int\\int(y(x + \\xi) - t)^2p(t | \\mathbf{x})p(\\mathbf{x}) p(\\xi){\\rm d}\\mathbf{x}{\\rm d}t {\\rm d}\\xi\n",
    "    \\end{equation}\n",
    "Using Taylor series expansion of $y(\\mathbf{x} + \\xi)$ show that\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} \\simeq E + \\lambda \\Omega\n",
    "    \\end{equation}\n",
    "where $\\Omega$ is a regularizer which takes the form of Tikhonov regularizer\n",
    "    \\begin{equation}\n",
    "    \\Omega = \\frac12 \\int \\|\\nabla y(\\mathbf{x})\\|^2 p(\\mathbf{x}){\\rm d} \\mathbf{x}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "# Есть в решении Димы  - никому не давать\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textrm{Proof:}}$\n",
    "\n",
    "First of all, use Taylor series expansion of $y(\\mathbf x + \\mathbf \\xi)$:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    y(\\mathbf x + \\mathbf \\xi) = y(\\mathbf x) + \\mathbf \\xi^T \\nabla y(\\mathbf x) + \\frac{1}{2} \\mathbf \\xi^T (\\nabla \\nabla y(\\mathbf x)) \\mathbf \\xi + \\mathcal{o}(\\|\\xi\\|^2), \\quad (4)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Then, plug (4) into (2), and omit terms with order higher than 2:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\tilde E = \\frac{1}{2}  \\iiint \\left(   y(\\mathbf x) - t + \\mathbf \\xi^T \\nabla y(\\mathbf x) + \\frac{1}{2} \\mathbf \\xi^T (\\nabla \\nabla y(\\mathbf x)) \\mathbf \\xi \\right) ^2 p(t|\\mathbf{x}) p(\\mathbf{x}) p(\\mathbf{\\xi}) d\\mathbf{x} dt d\\mathbf{\\xi} = \\\\\n",
    "    = \\frac{1}{2}  \\iiint \\left(   y(\\mathbf x) - t \\right)^2 p(t|\\mathbf{x}) p(\\mathbf{x}) p(\\mathbf{\\xi}) d\\mathbf{x} dt d\\mathbf{\\xi} \n",
    "    + \n",
    "    \\iiint \\left( (y(\\mathbf x) -t )(\\mathbf \\xi^T \\nabla y(\\mathbf x)+ \\frac{1}{2} \\mathbf \\xi^T (\\nabla \\nabla y(\\mathbf x)) \\mathbf \\xi) \n",
    "   \\right) p(t|\\mathbf{x}) p(\\mathbf{x}) p(\\mathbf{\\xi}) d\\mathbf{x} dt d\\mathbf{\\xi}\n",
    "    + \\\\ +\n",
    "    \\frac{1}{2} \\iiint \\left( \\nabla y(\\mathbf x)^T \\xi \\xi^T \\nabla y(\\mathbf x) \\right) ^2 p(t|\\mathbf{x}) p(\\mathbf{x}) p(\\mathbf{\\xi}) d\\mathbf{x} dt d\\mathbf{\\xi}, \\quad (5)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "If we take into account, than the mean value of $\\xi$ is zero, and $\\int p(\\xi) d\\xi = 1$, $\\int \\xi p(\\xi) d\\xi = 0$, we have that first term of (5) is $E$, second is approximately equal to zero (we assume that our model is pretty good):\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\tilde E = E\n",
    "    +\n",
    "    \\frac{1}{2} \\iiint \\left( \\nabla y(\\mathbf x)^T \\xi \\xi^T \\nabla y(\\mathbf x) \\right) ^2 p(t|\\mathbf{x}) p(\\mathbf{x}) p(\\mathbf{\\xi}) d\\mathbf{x} dt d\\mathbf{\\xi}, \\quad (6)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Since, $\\xi$ has a unit covarience matrix, we finally get that:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\tilde E = E\n",
    "    +\n",
    "    \\frac{1}{2} \\iiint \\left( \\nabla y(\\mathbf x)^T I \\nabla y(\\mathbf x) \\right) ^2 p(t|\\mathbf{x}) p(\\mathbf{x}) d\\mathbf{x} dt \n",
    "    = E + \n",
    "    \\frac{1}{2} \\iiint \\| \\nabla y(\\mathbf x)\\| ^2 p(t|\\mathbf{x}) p(\\mathbf{x}) d\\mathbf{x} dt\n",
    "    = E + \\lambda\n",
    "    \\frac{1}{2} \\iiint \\| \\nabla y(\\mathbf x)\\| ^2 p(\\mathbf{x}) d\\mathbf{x} =\\\\= E + \\lambda \\Omega. \\quad (7)\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
